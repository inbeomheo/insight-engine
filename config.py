"""
ìŠ¤ë§ˆíŠ¸ ì½˜í…ì¸  ìƒì„±ê¸° ì„¤ì • íŒŒì¼
AI ì„œë¹„ìŠ¤, ìŠ¤íƒ€ì¼ ì˜µì…˜ ì •ì˜
í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ prompts.pyì—ì„œ ê´€ë¦¬
"""
from typing import Dict, List, Any
import os

# prompts.pyì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
from prompts import STYLE_PROMPTS

# YouTube API Key
YOUTUBE_API_KEY: str = os.getenv('YOUTUBE_API_KEY', '')

# AI Provider API Keys (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
PROVIDER_API_KEYS: Dict[str, str] = {
    'openai': os.getenv('OPENAI_API_KEY', ''),
    'anthropic': os.getenv('ANTHROPIC_API_KEY', ''),
    'gemini': os.getenv('GEMINI_API_KEY', ''),
    'zhipu': os.getenv('ZHIPU_API_KEY', ''),
    'deepseek': os.getenv('DEEPSEEK_API_KEY', ''),
}

# Supadata API Key (YouTube ìë§‰ ë°±ì—… ì„œë¹„ìŠ¤)
SUPADATA_API_KEY: str = os.getenv('SUPADATA_API_KEY', '')

# Token Limits (ê¸°ë³¸ê°’, ëª¨ë¸ë³„ ì„¤ì •ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
MAX_TRANSCRIPT_TOKENS: int = 100000
MAX_COMMENTS_TOKENS: int = 5000
MAX_CONTENT_TOKENS: int = 100000  # ê¸°ë³¸ fallback ê°’

# ì§€ì› AI ì„œë¹„ìŠ¤ ì •ì˜ (max_input_tokens: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì˜ ~75% í• ë‹¹)
SUPPORTED_PROVIDERS: Dict[str, Dict[str, Any]] = {
    'gemini': {
        'name': 'Google Gemini',
        'models': [
            {'id': 'gemini/gemini-2.0-flash-lite', 'name': 'Gemini 2.0 Flash Lite', 'max_input_tokens': 750000},
            {'id': 'gemini/gemini-flash-lite-latest', 'name': 'Gemini Flash Lite (Latest)', 'max_input_tokens': 750000},
        ]
    },
    'deepseek': {
        'name': 'DeepSeek',
        'models': [
            {'id': 'deepseek/deepseek-chat', 'name': 'DeepSeek-V3 (ì±„íŒ…)', 'max_input_tokens': 96000},
            {'id': 'deepseek/deepseek-reasoner', 'name': 'DeepSeek-R1 (ì¶”ë¡ )', 'max_input_tokens': 96000}
        ]
    }
}


def get_available_providers() -> Dict[str, Dict[str, Any]]:
    """API í‚¤ê°€ ì„¤ì •ëœ í”„ë¡œë°”ì´ë”ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    available = {}
    for provider_id, api_key in PROVIDER_API_KEYS.items():
        if api_key and provider_id in SUPPORTED_PROVIDERS:
            available[provider_id] = SUPPORTED_PROVIDERS[provider_id]
    return available


def get_provider_from_model(model_id: str) -> str:
    """ëª¨ë¸ IDì—ì„œ í”„ë¡œë°”ì´ë”ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if model_id.startswith('gpt-'):
        return 'openai'
    elif model_id.startswith('claude-'):
        return 'anthropic'
    elif model_id.startswith('gemini/'):
        return 'gemini'
    elif model_id.startswith('glm-'):
        return 'zhipu'
    elif model_id.startswith('deepseek/'):
        return 'deepseek'
    return 'openai'  # ê¸°ë³¸ê°’

# ìŠ¤íƒ€ì¼/í†¤ ì˜µì…˜
STYLE_OPTIONS: List[tuple] = [
    ('blog', 'ğŸ“ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸'),
    ('detailed', 'ğŸ“ ìƒì„¸ ìš”ì•½'),
    ('summary', 'âš¡ í•µì‹¬ ìš”ì•½'),
    ('easy', 'ğŸ¯ ì‰¬ìš´ ì„¤ëª…'),
    ('news', 'ğŸ“° ë‰´ìŠ¤ ìŠ¤íƒ€ì¼'),
    ('script', 'ğŸ¬ ìŠ¤í¬ë¦½íŠ¸'),
    ('seo', 'ğŸ” SEO ìµœì í™”'),
    ('needs', 'ğŸ’¡ ë‹ˆì¦ˆ/ì•„ì´í…œ ë¶„ì„'),
    ('qna', 'â“ Q&A í˜•ì‹'),
    ('infographic', 'ğŸ“Š ì¸í¬ê·¸ë˜í”½ìš©'),
    ('compare', 'âš–ï¸ ë¹„êµë¶„ì„'),
    ('sns', 'ğŸ“± SNS í¬ìŠ¤íŒ…')
]

# ì„¸ë¶€ ì˜µì…˜ (ê¸¸ì´, í†¤, ì–¸ì–´, ì´ëª¨ì§€)
STYLE_MODIFIERS: Dict[str, Dict[str, str]] = {
    'length': {
        'short': 'ì´ ë¶„ëŸ‰ì€ ì•½ 300ì ë‚´ì™¸ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.',
        'medium': 'ì´ ë¶„ëŸ‰ì€ ì•½ 800ì ë‚´ì™¸ë¡œ ì ì ˆíˆ ì‘ì„±í•˜ì„¸ìš”.',
        'long': 'ì´ ë¶„ëŸ‰ì€ ì•½ 1500ì ì´ìƒìœ¼ë¡œ ìƒì„¸í•˜ê³  í’ë¶€í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.'
    },
    'tone': {
        'professional': 'ë§íˆ¬ëŠ” ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê²½ì–´ì²´(~ìŠµë‹ˆë‹¤, ~ì…ë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.',
        'friendly': 'ë§íˆ¬ëŠ” ì¹œê·¼í•˜ê³  ëŒ€í™”í•˜ë“¯ì´ ì‘ì„±í•˜ì„¸ìš”. ë¶€ë“œëŸ¬ìš´ ì–´ì¡°ë¡œ ë…ìì™€ ì†Œí†µí•˜ëŠ” ëŠë‚Œì„ ì£¼ì„¸ìš”.',
        'humorous': 'ë§íˆ¬ëŠ” ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê³  ì¬ì¹˜ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”. ì ì ˆí•œ ìœ„íŠ¸ì™€ ê°€ë²¼ìš´ ë†ë‹´ì„ í¬í•¨í•˜ì„¸ìš”.'
    },
    'language': {
        'ko': 'ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.',
        'en': 'Please write the result in English.',
        'ja': 'çµæœã¯å¿…ãšæ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚'
    },
    'emoji': {
        'use': 'ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ê°€ë…ì„±ê³¼ ì¬ë¯¸ë¥¼ ë†’ì´ì„¸ìš”. ê° ì„¹ì…˜ì´ë‚˜ ì¤‘ìš” í¬ì¸íŠ¸ì— ì´ëª¨ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.',
        'none': 'ì´ëª¨ì§€ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.'
    }
}

# STYLE_PROMPTSëŠ” prompts.pyì—ì„œ importë¨

def get_model_max_tokens(model_id: str) -> int:
    """ëª¨ë¸ IDë¡œ ìµœëŒ€ ì…ë ¥ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    for provider in SUPPORTED_PROVIDERS.values():
        for model in provider.get('models', []):
            if model['id'] == model_id:
                return model.get('max_input_tokens', MAX_CONTENT_TOKENS)
    return MAX_CONTENT_TOKENS  # fallback

__all__ = [
    'YOUTUBE_API_KEY',
    'PROVIDER_API_KEYS',
    'SUPADATA_API_KEY',
    'MAX_TRANSCRIPT_TOKENS',
    'MAX_COMMENTS_TOKENS',
    'MAX_CONTENT_TOKENS',
    'SUPPORTED_PROVIDERS',
    'STYLE_OPTIONS',
    'STYLE_MODIFIERS',
    'STYLE_PROMPTS',
    'get_model_max_tokens',
    'get_available_providers',
    'get_provider_from_model'
]
